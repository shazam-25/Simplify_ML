{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC1v71bpBu23AusxhvWjMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shazam-25/Simplify_ML/blob/main/MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QPwCMtVtb16",
        "outputId": "63d3768b-3c32-4420-c6af-2d5c42b0e5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Value function = [10.  2.  0.]\n",
            "Iteration 2: Value function = [12.56  2.    0.  ]\n",
            "Iteration 3: Value function = [14.4032  2.      0.    ]\n",
            "Iteration 4: Value function = [15.730304  2.        0.      ]\n",
            "Iteration 5: Value function = [16.68581888  2.          0.        ]\n",
            "Iteration 6: Value function = [17.37378959  2.          0.        ]\n",
            "Iteration 7: Value function = [17.86912851  2.          0.        ]\n",
            "Iteration 8: Value function = [18.22577253  2.          0.        ]\n",
            "Iteration 9: Value function = [18.48255622  2.          0.        ]\n",
            "Iteration 10: Value function = [18.66744048  2.          0.        ]\n",
            "\n",
            "Optimal value function: [18.66744048  2.          0.        ]\n",
            "Optimal policy (action per state): [0 1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simple MDP setup\n",
        "states = [0,1,2]  # Set of states S\n",
        "actions = [0,1]   # Set of actions A\n",
        "gamma = 0.9       # Discount factor\n",
        "\n",
        "# Transition probabilities: P[s,a,s']\n",
        "P = np.array([\n",
        "    [[0.8, 0.2, 0.0], [0.0, 1.0, 0.0]],   # From state 0\n",
        "    [[0.0, 0.9, 0.1], [0.0, 0.0, 1.0]],   # From state 1\n",
        "    [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]    # From state 2\n",
        "])\n",
        "\n",
        "# Rewards: R[s,a]\n",
        "R = np.array([\n",
        "    [5, 10],    # State 0\n",
        "    [-1, 2],    # State 1\n",
        "    [0, 0]      # State 2\n",
        "])\n",
        "\n",
        "# Value Iteration\n",
        "V = np.zeros(len(states))\n",
        "num_iterations = 10\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  V_new = np.zeros_like(V)\n",
        "  for s in states:\n",
        "    V_new[s] = max(sum(P[s,a,s_next]*(R[s,a]+gamma*V[s_next]) for s_next in states) for a in actions)\n",
        "    V = V_new\n",
        "  print(f'Iteration {i+1}: Value function = {V}')\n",
        "\n",
        "# Compute optimal policy\n",
        "policy = np.zeros(len(states), dtype=int)\n",
        "for s in states:\n",
        "  policy[s] = np.argmax([sum(P[s,a,s_next]*(R[s,a]+gamma*V[s_next]) for s_next in states) for a in actions])\n",
        "\n",
        "print('\\nOptimal value function:', V)\n",
        "print('Optimal policy (action per state):', policy)"
      ]
    }
  ]
}